<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Spark | 邹斌的博客</title>
  <meta name="keywords" content=" 大数据 , 推荐系统 , Hadoop ">
  <meta name="description" content="Spark | 邹斌的博客">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="[TOC] 遗传算法使用二进制初始化-&gt;选择算子-&gt;交叉算子-&gt;变异算子-&gt;选择算子 初始化基本参数群体大小20-100进化代数100-500交叉概率0.4-0.99变异概率取0.0001-0.1这4个运行参数对遗传算法的结果和求解效率都有一定影响，但是没有理论依据，需要进行多次试算才能确定参数合理大小和取值范围 基本遗传算法定义$$SGA&#x3D;(C, E, P_0, M, \">
<meta property="og:type" content="article">
<meta property="og:title" content="遗传算法">
<meta property="og:url" content="https://bufan-zb.github.io/blog/2020/12/22/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="邹斌的博客">
<meta property="og:description" content="[TOC] 遗传算法使用二进制初始化-&gt;选择算子-&gt;交叉算子-&gt;变异算子-&gt;选择算子 初始化基本参数群体大小20-100进化代数100-500交叉概率0.4-0.99变异概率取0.0001-0.1这4个运行参数对遗传算法的结果和求解效率都有一定影响，但是没有理论依据，需要进行多次试算才能确定参数合理大小和取值范围 基本遗传算法定义$$SGA&#x3D;(C, E, P_0, M, \">
<meta property="og:locale">
<meta property="article:published_time" content="2020-12-22T10:58:00.000Z">
<meta property="article:modified_time" content="2022-01-22T05:51:02.073Z">
<meta property="article:author" content="邹斌">
<meta property="article:tag" content="遗传算法">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/avatar.jpg">

<link href="/blog/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/blog/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/blog/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/blog/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 5.3.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="/blog">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/blog/"
   class="avatar_target">
    <img class="avatar"
         src="/blog/img/avatar.jpg"/>
</a>
<div class="author">
    <span>邹斌</span>
</div>

<div class="icon">
    
        
            <a title="github"
               href="https://github.com/bufan-zb"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-github"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="email"
               href="mailto:zoubinbf@163.com"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-email"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="qq"
               href="http://wpa.qq.com/msgrd?v=3&uin=592916444&site=qq&menu=yes"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-qq"></use>
                    </svg>
                
            </a>
        
    
</div>




<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(60)</small>
            
        </div>
    </li>
    
        
            
                <li>
                    <div data-rel="大数据">
                        
                        大数据
                        <small>(10)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="排序算法">
                        
                        排序算法
                        <small>(10)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="人工智能">
                        
                        人工智能
                        <small>(12)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="数据结构">
                        
                        数据结构
                        <small>(4)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="Docker">
                        
                        Docker
                        <small>(1)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="Linux">
                        
                        Linux
                        <small>(12)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="python">
                        
                        python
                        <small>(10)</small>
                        
                    </div>
                    
                </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
            
            
    </div>
    <div>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="60">
<input type="hidden" id="yelog_site_word_count" value="65.1k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" />
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>测试</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>大数据</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>聊天</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>模型推导</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>排序算法</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>数据分析</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>数学</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>搜索</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>搜索引擎</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>算法</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>梯度下降</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>推荐系统</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>网卡配置</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>未完成</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>虚拟环境</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>遗传算法</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>cenos</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Docker</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Elasticsearch</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>FastDFS</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>flask</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Gitblit</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Hadoop</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Kafka</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Linux</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Linux常用命令</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>MySQL</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Numpy</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Pandas</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>python</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Python</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>pytorch</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Pytorch</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>RNN</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>TensorFlow</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>web</a>
            </li>
        
    </div>

</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a  class="全部文章 人工智能 "
           href="/blog/2020/12/22/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/"
           data-tag="遗传算法"
           data-author="" >
            <span class="post-title" title="遗传算法">遗传算法</span>
            <span class="post-date" title="2020-12-22 18:58:00">2020/12/22</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2020/08/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"
           data-tag="模型推导"
           data-author="" >
            <span class="post-title" title="逻辑回归">逻辑回归</span>
            <span class="post-date" title="2020-08-12 18:58:00">2020/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/18/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"
           data-tag="算法,梯度下降"
           data-author="" >
            <span class="post-title" title="优化算法">优化算法</span>
            <span class="post-date" title="2019-08-18 18:58:00">2019/08/18</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/Docker%E9%83%A8%E7%BD%B2%20FastDFS/"
           data-tag="Docker,FastDFS"
           data-author="" >
            <span class="post-title" title="Docker部署FastDFS">Docker部署FastDFS</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/Docker%E9%83%A8%E7%BD%B2Elasticsearch/"
           data-tag="Docker,Elasticsearch"
           data-author="" >
            <span class="post-title" title="Docker部署Elasticsearch">Docker部署Elasticsearch</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/Docker%E9%83%A8%E7%BD%B2MySQL/"
           data-tag="Docker,MySQL"
           data-author="" >
            <span class="post-title" title="Docker部署MySQL">Docker部署MySQL</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/Dockre%E9%83%A8%E7%BD%B2Gitblit/"
           data-tag="Docker,Gitblit"
           data-author="" >
            <span class="post-title" title="Docker部署Gitblit">Docker部署Gitblit</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/ES%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/"
           data-tag="Elasticsearch,搜索"
           data-author="" >
            <span class="post-title" title="ES部署">ES部署</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/FM/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="FM">FM</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/FTRL/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="FTRL">FTRL</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Docker "
           href="/blog/2019/08/12/GPU%E9%95%9C%E5%83%8F%E7%94%9F%E6%88%90/"
           data-tag="Pytorch,TensorFlow"
           data-author="" >
            <span class="post-title" title="GPU镜像生成">GPU镜像生成</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/Flume/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="Flume">Flume</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/GRU/"
           data-tag="RNN"
           data-author="" >
            <span class="post-title" title="GRU">GRU</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/HBase/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="HBase">HBase</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/HDFS/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="HDFS">HDFS</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/Hadoop/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="Hadoop">Hadoop</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="K-近邻算法">K-近邻算法</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/Kafka/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="Kafka">Kafka</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/Kafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"
           data-tag="大数据,推荐系统,Kafka"
           data-author="" >
            <span class="post-title" title="Kafka 集群搭建">Kafka 集群搭建</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/LSTM/"
           data-tag="RNN"
           data-author="" >
            <span class="post-title" title="LSTM">LSTM</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/MapReduce/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="MapReduce">MapReduce</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/Linux%E5%91%BD%E4%BB%A4/"
           data-tag="Linux常用命令"
           data-author="" >
            <span class="post-title" title="Linux命令">Linux命令</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/N-garm/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="N-Garm">N-Garm</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2019/08/12/Numpy/"
           data-tag="数据分析,Numpy"
           data-author="" >
            <span class="post-title" title="Numpy">Numpy</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/Spark/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="Spark">Spark</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/Sqoop/"
           data-tag="未完成,大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="Sqoop">Sqoop</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2019/08/12/Vim/"
           data-tag="Linux"
           data-author="" >
            <span class="post-title" title="Vim">Vim</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2019/08/12/Tensorflow/"
           data-tag="TensorFlow"
           data-author="" >
            <span class="post-title" title="TensorFlow">TensorFlow</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 大数据 "
           href="/blog/2019/08/12/YARN/"
           data-tag="大数据,推荐系统,Hadoop"
           data-author="" >
            <span class="post-title" title="YARN">YARN</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2019/08/12/hdfs%E8%BF%9E%E6%8E%A5/"
           data-tag="Hadoop"
           data-author="" >
            <span class="post-title" title="hdfs连接">hdfs连接</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/Word2Vec/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="Word2Vec">Word2Vec</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2019/08/12/pandas/"
           data-tag="数据分析,Pandas"
           data-author="" >
            <span class="post-title" title="Pandas">Pandas</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2019/08/12/hive%E8%BF%9E%E6%8E%A5/"
           data-tag="Hadoop"
           data-author="" >
            <span class="post-title" title="hive连接">hive连接</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2019/08/12/pytorch/"
           data-tag="pytorch"
           data-author="" >
            <span class="post-title" title="pytorch">pytorch</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="冒泡排序">冒泡排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="基数排序">基数排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="希尔排序">希尔排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E5%A0%86%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="堆排序">堆排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="损失函数">损失函数</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="快速排序">快速排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="归并排序">归并排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="插入排序">插入排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 "
           href="/blog/2019/08/12/%E6%95%B0%E5%AD%A6/"
           data-tag="数学"
           data-author="" >
            <span class="post-title" title="数学">数学</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 数据结构 "
           href="/blog/2019/08/12/%E6%A0%88/"
           data-tag="Python"
           data-author="" >
            <span class="post-title" title="栈">栈</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E6%A1%B6%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="桶排序">桶排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 数据结构 "
           href="/blog/2019/08/12/%E6%A0%91/"
           data-tag="Python"
           data-author="" >
            <span class="post-title" title="树">树</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 人工智能 "
           href="/blog/2019/08/12/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"
           data-tag="算法"
           data-author="" >
            <span class="post-title" title="激活函数">激活函数</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="计数排序">计数排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 数据结构 "
           href="/blog/2019/08/12/%E9%98%9F%E5%88%97/"
           data-tag="Python"
           data-author="" >
            <span class="post-title" title="队列">队列</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 排序算法 "
           href="/blog/2019/08/12/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/"
           data-tag="排序算法"
           data-author="" >
            <span class="post-title" title="选择排序">选择排序</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 数据结构 "
           href="/blog/2019/08/12/%E9%93%BE%E8%A1%A8/"
           data-tag="Python"
           data-author="" >
            <span class="post-title" title="链表">链表</span>
            <span class="post-date" title="2019-08-12 18:58:00">2019/08/12</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2018/06/02/supervisor/"
           data-tag="web,python"
           data-author="" >
            <span class="post-title" title="supervisor进程管理">supervisor进程管理</span>
            <span class="post-date" title="2018-06-02 15:58:00">2018/06/02</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2018/05/17/unittest/"
           data-tag="web,测试"
           data-author="" >
            <span class="post-title" title="单元测试">单元测试</span>
            <span class="post-date" title="2018-05-17 19:58:00">2018/05/17</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2018/05/02/web%E9%83%A8%E7%BD%B2/"
           data-tag="web,flask"
           data-author="" >
            <span class="post-title" title="部署flask项目全流程">部署flask项目全流程</span>
            <span class="post-date" title="2018-05-02 21:58:00">2018/05/02</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2017/08/12/crontab/"
           data-tag="web,cenos"
           data-author="" >
            <span class="post-title" title="crontab定时任务">crontab定时任务</span>
            <span class="post-date" title="2017-08-12 18:58:00">2017/08/12</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2017/05/13/git/"
           data-tag="cenos"
           data-author="" >
            <span class="post-title" title="git">git</span>
            <span class="post-date" title="2017-05-13 18:58:00">2017/05/13</span>
        </a>
        
        <a  class="全部文章 python "
           href="/blog/2017/05/13/socketio/"
           data-tag="web,聊天"
           data-author="" >
            <span class="post-title" title="Socketio即时通讯">Socketio即时通讯</span>
            <span class="post-date" title="2017-05-13 14:58:00">2017/05/13</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2017/05/10/network/"
           data-tag="cenos,网卡配置"
           data-author="" >
            <span class="post-title" title="Cenos配置静态IP">Cenos配置静态IP</span>
            <span class="post-date" title="2017-05-10 15:58:00">2017/05/10</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2017/04/23/miniconda/"
           data-tag="虚拟环境"
           data-author="" >
            <span class="post-title" title="miniconda">miniconda</span>
            <span class="post-date" title="2017-04-23 17:27:00">2017/04/23</span>
        </a>
        
        <a  class="全部文章 Linux "
           href="/blog/2017/04/22/Elasticsearch/"
           data-tag="web,搜索引擎,未完成"
           data-author="" >
            <span class="post-title" title="Elasticsearch">Elasticsearch</span>
            <span class="post-date" title="2017-04-22 19:27:00">2017/04/22</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-Spark" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">Spark</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            <i class="iconfont icon-category"></i>
            
            
            <a  data-rel="大数据">大数据</a>
            
        </span>
        
        
        <span class="tag">
            <i class="iconfont icon-tag"></i>
            
            <a class="color4">大数据</a>
            
            <a class="color5">推荐系统</a>
            
            <a class="color2">Hadoop</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title='最后更新: 2022-01-22 14:13:09'>2019-08-12 18:58</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:5.4k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E7%AE%80%E4%BB%8B"><span class="toc-text">Spark简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E6%A6%82%E8%BF%B0"><span class="toc-text">Spark概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E6%A6%82%E8%BF%B0"><span class="toc-text">RDD概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BARDD"><span class="toc-text">创建RDD</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="toc-text">RDD常用算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%B1%BBRDD%E7%AE%97%E5%AD%90"><span class="toc-text">三类RDD算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformation%E7%AE%97%E5%AD%90"><span class="toc-text">Transformation算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action%E7%AE%97%E5%AD%90"><span class="toc-text">Action算子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E9%9B%86%E7%BE%A4"><span class="toc-text">Spark集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%EF%BC%88Standalone%E6%A8%A1%E5%BC%8F%EF%BC%89"><span class="toc-text">spark集群架构（Standalone模式）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL-%E6%A6%82%E8%BF%B0"><span class="toc-text">Spark SQL 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL%E6%A6%82%E5%BF%B5"><span class="toc-text">Spark SQL概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL%E5%8E%86%E5%8F%B2"><span class="toc-text">Spark SQL历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL%E4%BC%98%E5%8A%BF"><span class="toc-text">Spark SQL优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSQL%E4%BC%98%E7%82%B9"><span class="toc-text">SparkSQL优点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DataFrame"><span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame-vs-RDD"><span class="toc-text">DataFrame vs RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pandas-DataFrame-vs-Spark-DataFrame"><span class="toc-text">Pandas DataFrame  vs  Spark DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BADataFrame"><span class="toc-text">创建DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-text">优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4Python%E7%8E%AF%E5%A2%83%E5%88%B0spark%E9%9B%86%E7%BE%A4%E6%89%A7%E8%A1%8CPython%E8%84%9A%E6%9C%AC"><span class="toc-text">提交Python环境到spark集群执行Python脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkStreaming"><span class="toc-text">SparkStreaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming%E6%A6%82%E8%BF%B0"><span class="toc-text">SparkStreaming概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94"><span class="toc-text">实时计算框架对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming%E7%BB%84%E4%BB%B6"><span class="toc-text">SparkStreaming组件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-Streaming%E7%9A%84%E7%8A%B6%E6%80%81%E6%93%8D%E4%BD%9C"><span class="toc-text">Spark Streaming的状态操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#updateStateByKey"><span class="toc-text">updateStateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Windows"><span class="toc-text">Windows</span></a></li></ol></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><div class='inner-toc'><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E7%AE%80%E4%BB%8B"><span class="toc-text">Spark简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E6%A6%82%E8%BF%B0"><span class="toc-text">Spark概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E6%A6%82%E8%BF%B0"><span class="toc-text">RDD概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BARDD"><span class="toc-text">创建RDD</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="toc-text">RDD常用算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%B1%BBRDD%E7%AE%97%E5%AD%90"><span class="toc-text">三类RDD算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformation%E7%AE%97%E5%AD%90"><span class="toc-text">Transformation算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action%E7%AE%97%E5%AD%90"><span class="toc-text">Action算子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E9%9B%86%E7%BE%A4"><span class="toc-text">Spark集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%EF%BC%88Standalone%E6%A8%A1%E5%BC%8F%EF%BC%89"><span class="toc-text">spark集群架构（Standalone模式）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL-%E6%A6%82%E8%BF%B0"><span class="toc-text">Spark SQL 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL%E6%A6%82%E5%BF%B5"><span class="toc-text">Spark SQL概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL%E5%8E%86%E5%8F%B2"><span class="toc-text">Spark SQL历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL%E4%BC%98%E5%8A%BF"><span class="toc-text">Spark SQL优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSQL%E4%BC%98%E7%82%B9"><span class="toc-text">SparkSQL优点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DataFrame"><span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame-vs-RDD"><span class="toc-text">DataFrame vs RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pandas-DataFrame-vs-Spark-DataFrame"><span class="toc-text">Pandas DataFrame  vs  Spark DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BADataFrame"><span class="toc-text">创建DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-text">优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4Python%E7%8E%AF%E5%A2%83%E5%88%B0spark%E9%9B%86%E7%BE%A4%E6%89%A7%E8%A1%8CPython%E8%84%9A%E6%9C%AC"><span class="toc-text">提交Python环境到spark集群执行Python脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkStreaming"><span class="toc-text">SparkStreaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming%E6%A6%82%E8%BF%B0"><span class="toc-text">SparkStreaming概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94"><span class="toc-text">实时计算框架对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming%E7%BB%84%E4%BB%B6"><span class="toc-text">SparkStreaming组件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-Streaming%E7%9A%84%E7%8A%B6%E6%80%81%E6%93%8D%E4%BD%9C"><span class="toc-text">Spark Streaming的状态操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#updateStateByKey"><span class="toc-text">updateStateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Windows"><span class="toc-text">Windows</span></a></li></ol></li></ol></li></ol></div></p>
<h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><h3 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h3><ul>
<li>什么是Spark<ul>
<li>基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储</li>
<li>Spark的优势<ul>
<li><strong>MapReduce框架局限性</strong><ul>
<li>Map结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据</li>
<li>任务调度和启动开销大</li>
<li>无法充分利用内存</li>
<li>不适合迭代计算（如机器学习、图计算等等），交互式处理（数据挖掘）</li>
<li>不适合流式处理（点击日志分析）</li>
<li>MapReduce编程不够灵活，仅支持Map和Reduce两种操作</li>
</ul>
</li>
<li><strong>Hadoop生态圈</strong><ul>
<li>批处理：MapReduce、Hive、Pig</li>
<li>流式计算：Storm</li>
<li>交互式计算：impala、presto</li>
</ul>
</li>
<li><strong>需要一种灵活的框架可同时进行批处理、流式计算、交互式计算</strong><ul>
<li>内存计算引擎，提供cache机制来支持需要反复迭代计算或者多次数据共享，减少数据读取的IO开销</li>
<li>DAG引擎，较少多次计算之间中间结果写到HDFS的开销</li>
<li>使用多线程模型来减少task启动开销，shuffle过程中避免不必要的sort操作以及减少磁盘IO</li>
</ul>
</li>
<li>spark的缺点：吃内存，不太稳定</li>
<li><strong>Spark特点</strong><ul>
<li>速度快（比MapReduce在内存中快100倍，在磁盘中快10倍）<ul>
<li>spaek中的job中间结果可以不落地，可以存放在内存中</li>
<li>MapReduce中Map和Reduce任务都是以进程的方式运行着，而spark中的job是以线程方式运行在进程中</li>
</ul>
</li>
<li>易用性（可以通过java/scala/python/R开发spark应用）</li>
<li>通用性（可以使用spark sql/spark streaming/mlib/Graphx）</li>
<li>兼容性（spark程序可以运行在standalone/yarn/mesos）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h3><ul>
<li>什么是RDD<ul>
<li>RDD(Resilient Distributed Dataset)叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算集合<ul>
<li>Dataset：一个数据集，简单的理解为集合，用于存放数据的</li>
<li>Distributed：它的数据是分布式存储，并且可以做分布式计算</li>
<li>Resilient：弹性的<ul>
<li>它表示的是数据可以保存在磁盘，也可以保存在内存中</li>
<li>数据分布式也是弹性的</li>
<li>弹性：并不是指它可以动态扩展，而是容错机制<ul>
<li>RDD会在多个节点上存储，就和HDFS得分布式道理一样的。HDFS文件被切分为多个block存储在各个节点上，而RDD是被切分为多个partition。不同的partition可能在不同节点上</li>
<li>spark读取HDFS的场景下，spark把HDFS的block读到内存就会抽象为spark的partition。</li>
<li>spark计算结束，一般会把数据做持久化到Hive，HBase，HDFS等等</li>
</ul>
</li>
</ul>
</li>
<li>不可变：Rdd数据不可变，只能是生成一个新的Rdd</li>
<li>可分区partition</li>
<li>并行计算</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h3><ul>
<li><p>第一步创建sparkContext</p>
<ul>
<li>SparkContext，Spark程序入口。SparkContext代表了和Spark集群的链接，在Spark集群中通过SparkContext来创建RDD</li>
<li>SparkConf创建SparkContext的时候需要一个SparkConf，用来传递Spark应用的基本信息</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf &#x3D; SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br></pre></td></tr></table></figure></li>
<li><p>创建RDD</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; [1, 2, 3, 4, 5]</span><br><span class="line"># 创建RDD</span><br><span class="line">data_rdd &#x3D; sc.parallelize(data)</span><br><span class="line"># 创建RDD，并且分成5个分区</span><br><span class="line">data_rdd &#x3D; sc.parallelize(data, 5)</span><br><span class="line"># 直接读取文件生成RDD</span><br><span class="line">data_rdd &#x3D; sc.textFile(&#39;路径&#39;)</span><br></pre></td></tr></table></figure>
<h1 id="RDD常用算子"><a href="#RDD常用算子" class="headerlink" title="RDD常用算子"></a>RDD常用算子</h1><h3 id="三类RDD算子"><a href="#三类RDD算子" class="headerlink" title="三类RDD算子"></a>三类RDD算子</h3><ul>
<li>transformation算子（该算子操作都是惰性的，不会立即计算出结果，会记录计算过程，只有在进行action操作才会计算结果）<ul>
<li>从一个已经存在的数据集创建一个新的数据集<ul>
<li>rdd_a —–&gt;transformation—–&gt; rdd_b</li>
</ul>
</li>
</ul>
</li>
<li>action算子<ul>
<li>获取对数据进行运算操作之后的结果</li>
</ul>
</li>
<li>persist操作算子<ul>
<li>persist操作用于将数据缓存，可以缓存在内存中，也可以在磁盘上，也可以复制到磁盘的其它节点上</li>
</ul>
</li>
</ul>
<h3 id="Transformation算子"><a href="#Transformation算子" class="headerlink" title="Transformation算子"></a>Transformation算子</h3><ul>
<li><p>map</p>
<ul>
<li>将func函数作用到数据集的每一个元素上，生成一个新的RDD返回</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line">rdd2 = rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x+<span class="number">1</span>)</span><br><span class="line">rdd2.collect()</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>filter</p>
<ul>
<li>选出所有func返回值为true的元素，生成一个新的RDD返回</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line">rdd2 = rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">rdd3 = rdd2.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x&gt;<span class="number">4</span>)</span><br><span class="line">rdd3.collect()</span><br><span class="line">[<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>flatMap</p>
<ul>
<li>flatMap会先执行map操作，再将所有对象合并为一个对象</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="string">&quot;a b c&quot;</span>, <span class="string">&quot;d e f&quot;</span>, <span class="string">&quot;h i j&quot;</span>])</span><br><span class="line">rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">rdd2.collect()</span><br><span class="line">[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;j&#x27;</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>union</p>
<ul>
<li>对两个RDD求并集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)])</span><br><span class="line">rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3.collect()</span><br><span class="line">[(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)]</span><br></pre></td></tr></table></figure></li>
<li><p>intersection</p>
<ul>
<li>对两个RDD求交集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)])</span><br><span class="line">rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd4 = rdd3.intersection(rdd2)</span><br><span class="line">rdd4.collect()</span><br><span class="line">[(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)]</span><br></pre></td></tr></table></figure></li>
<li><p>groupByKey</p>
<ul>
<li>以元组中的第0个元素作为key，进行分组，返回一个新的RDD</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;c&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">3</span>)])</span><br><span class="line">rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd4 = rdd3.groupByKey()</span><br><span class="line">result = rdd4.collect()</span><br><span class="line">result</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, &lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6a5e5898</span>&gt;), (<span class="string">&#x27;c&#x27;</span>, &lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6a5e5518</span>&gt;), (<span class="string">&#x27;b&#x27;</span>, &lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6a5e5f28</span>&gt;)]</span><br><span class="line">result[<span class="number">2</span>]</span><br><span class="line">(<span class="string">&#x27;b&#x27;</span>, &lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6c18e518</span>&gt;)</span><br><span class="line">result[<span class="number">2</span>][<span class="number">1</span>]</span><br><span class="line">&lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6c18e518</span>&gt;</span><br><span class="line"><span class="built_in">list</span>(result[<span class="number">2</span>][<span class="number">1</span>])</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>reduceByKey</p>
<ul>
<li>将key相同的键值，按照func进行计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> a,y: x+y).collect()</span><br><span class="line">[(<span class="string">&#x27;b&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="Action算子"><a href="#Action算子" class="headerlink" title="Action算子"></a>Action算子</h3></li>
<li><p>collect</p>
<ul>
<li>返回一个list，list中包含RDD中的所有元素</li>
<li>只有当数据量较小的时候使用Collect因为所有的结果都会加载到内存中</li>
</ul>
</li>
<li><p>reduce</p>
<ul>
<li>reduce将RDD中元素两两传递给输入函数，同时产生一个新的值，新产生的值与RDD中下一个元素再被传递给输入函数直到最后一个值为止。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd1 &#x3D; sc.parallelize([1, 2, 3, 4])</span><br><span class="line">rdd1.reduce(lambda x,y: x+y)</span><br><span class="line">15</span><br></pre></td></tr></table></figure></li>
<li><p>first</p>
<ul>
<li>返回RDD的第一个元素</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([3,4,5]).first()</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li>
<li><p>take</p>
<ul>
<li>返回RDD的前n个元素</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([2,3,4,5]).take(2)</span><br><span class="line">[2,3]</span><br></pre></td></tr></table></figure></li>
<li><p>count</p>
<ul>
<li>返回RDD中元素的个数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([1,3,5]).count()</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<h1 id="Spark集群"><a href="#Spark集群" class="headerlink" title="Spark集群"></a>Spark集群</h1></li>
</ul>
<h3 id="spark集群架构（Standalone模式）"><a href="#spark集群架构（Standalone模式）" class="headerlink" title="spark集群架构（Standalone模式）"></a>spark集群架构（Standalone模式）</h3><p><img src="/blog/img/Spark_1.png"></p>
<ul>
<li><p>Application</p>
<ul>
<li>用户自己写的Spark应用程序，批处理作业的集合。Application的main方法为应用程序的入口，用户通过Spark的API，定义了RDD和对RDD的操作</li>
</ul>
</li>
<li><p>Master和Worker</p>
<p>整个集群分为Master节点和Worker节点，相当于Hadoop的Master和Slave节点</p>
<ul>
<li>Master：Standalone模式中主控节点，负责接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。</li>
<li>Worker：Standalone模式中slave节点上的守护进程，负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，启动Driver和Executor</li>
</ul>
</li>
<li><p>Client：客户端进程，负责提交作业到Master。</p>
</li>
<li><p>Driver：一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGScheduler，TaskScheduler</p>
</li>
<li><p>Executor：即真正执行作业的地方，一个集群一般包含多个Executor，每个Executor接收Deiver的命令Launch Task，一个Executor可以执行一个到多个Task</p>
</li>
<li><p>Spark作业相关概念</p>
<ul>
<li>Stage：一个Spark作业一般包含一到多个Stage</li>
<li>Task：一个Stage包含一到多个Task，通过多个Task实现并行运行的功能</li>
<li>DAGScheduler：实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task放到TaskScheduler中。</li>
<li>TaskScheduler：实现Task分配到Executor上执行。</li>
</ul>
</li>
</ul>
<h1 id="Spark-SQL-概述"><a href="#Spark-SQL-概述" class="headerlink" title="Spark SQL 概述"></a>Spark SQL 概述</h1><h3 id="Spark-SQL概念"><a href="#Spark-SQL概念" class="headerlink" title="Spark SQL概念"></a>Spark SQL概念</h3><ul>
<li>Spark SQL是Apache Spark用于处理结构化数据的模块<ul>
<li>它是spark中用于处理结构化数据的一个模块</li>
</ul>
</li>
</ul>
<h3 id="Spark-SQL历史"><a href="#Spark-SQL历史" class="headerlink" title="Spark SQL历史"></a>Spark SQL历史</h3><ul>
<li>Hive是目前大数据领域，事实上的数据仓库标准</li>
<li>Shark：shark底层使用spark的基于内存的计算模型，从而让计算性能比Hive提升了数倍到上百倍。</li>
<li>底层很多东西还是依赖于Hive，修改了内存管理、物理计划、执行三个模块</li>
<li>2014年6月1日，spark宣布了不再开发Shark，全面转向Spark SQL的开发</li>
</ul>
<h3 id="Spark-SQL优势"><a href="#Spark-SQL优势" class="headerlink" title="Spark SQL优势"></a>Spark SQL优势</h3><p><img src="/blog/img/Spark_2.png"></p>
<ul>
<li>速度</li>
</ul>
<p><img src="/blog/img/Spark_3.png"></p>
<p>python操作RDD，转换为可执行代码，运行再java虚拟机，涉及两个不同语言引擎之间的切换，进行进程间通信很耗费性能。</p>
<p>DataFrame</p>
<ul>
<li>是RDD为基础的分布式数据集，类似于传统关系型数据库的二维表，dataframe记录了对应列的名称和类型</li>
<li>dataFrame引入schema和off-heap(使用操作系统层面上的内存)<ul>
<li>1.解决了RDD的缺点</li>
<li>序列化和反序列化开销大</li>
<li>频繁的创建和销毁对象造成大量的GC</li>
<li>2.丢失了RDD的优点</li>
<li>RDD编译时进行类型检查</li>
<li>RDD具有面向对象编程的特性</li>
</ul>
</li>
</ul>
<p>用scala/python编写的RDD比Spark SQL编写转换的RDD慢，涉及到执行计划</p>
<ul>
<li>CatalystOptimizer：Catalyst优化器</li>
<li>ProjectTungsten：钨丝计划，为了提高RDD的效率而制定的计划</li>
<li>Code gen：代码生成器</li>
</ul>
<p><img src="/blog/img/Spark_4.png"></p>
<p>直接编写RDD也可以自实现优化代码，但是远不及SparkSQL面前的优化操作后转换的RDD效率高，快1倍左右</p>
<p>优化引擎：类似mysql等关系型数据库基于成本的优化器</p>
<p>首先执行逻辑执行计划，然后转换为物理执行计划（选择成本最小的），通过Code Generation最终生成RDD</p>
<ul>
<li>Language-independent API</li>
</ul>
<p>用任何语言编写生成的RDD都一样，而使用spark-core编写的RDD，不同的语言生成不同的RDD</p>
<ul>
<li>Schema</li>
</ul>
<p>结构化数据，可以直接看出数据的详情</p>
<p>在RDD中无法看出，解释性不强，无法告诉引擎信息，没法详细优化</p>
<h3 id="SparkSQL优点"><a href="#SparkSQL优点" class="headerlink" title="SparkSQL优点"></a>SparkSQL优点</h3><ul>
<li>易整合</li>
<li>统一的数据源访问</li>
<li>兼容Hive</li>
<li>提供了标准的数据库连接</li>
</ul>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在Spark语义中，DataFrame是一个分布式的行集合，可以想象为一个关系型数据库的表。</p>
<ul>
<li>Immuatable：一旦RDD、DataFrame被创建，就不能更改，只能通过transformation生成新的RDD、DataFrame</li>
<li>Lazy Evaluations：只有action才会触发Transformation的执行</li>
<li>Distributed：DataFrame和RDD一样都是分布式的</li>
<li>dataframe和dataset统一，dataframe只是dataset[ROW]的类型别名。由于Python是弱类型语言，只能使用DataFrame</li>
</ul>
<h3 id="DataFrame-vs-RDD"><a href="#DataFrame-vs-RDD" class="headerlink" title="DataFrame vs RDD"></a>DataFrame vs RDD</h3><ul>
<li>RDD:分布式的对象的集合，Spark并不知道对象的详细模式信息</li>
<li>DataFrame：分布式的Row对象的集合，其提供了由列组成的详细模式信息，使得Spark SQL可以进行某些形式的执行优化。</li>
<li>DataFrame和普通的RDD的逻辑框架区别如下所示：</li>
</ul>
<p><img src="/blog/img/Spark_5.png"></p>
<ul>
<li>左侧的RDD Spark框架本身不了解Person类的内部结构</li>
<li>右侧的DataFrame提供了详细的结构信息（schema—每列的名称、类型）</li>
<li>DataFrame还配套了新的操作数据方法，DataFrame API（如：df.select()）和SQL(select id, name from ***)</li>
<li>DataFrame还引入了off-heap,意味着JVM堆以外的内存，这些内存直接受操作系统管理(而不是JVM)</li>
<li>RDD是分布式的java对象集合，DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化</li>
<li>DataFrame的抽象后，我们处理数据更加简单了，甚至可以用SQL来处理数据</li>
<li>通过DataFrame API或SQL处理数据，会自动经过Spark优化器(Catalyst)的优化，即使你写的程序或SQL不高效，也可以运行很快。</li>
<li>DataFrame相当于是一个带着schema的RDD</li>
</ul>
<h3 id="Pandas-DataFrame-vs-Spark-DataFrame"><a href="#Pandas-DataFrame-vs-Spark-DataFrame" class="headerlink" title="Pandas DataFrame  vs  Spark DataFrame"></a>Pandas DataFrame  vs  Spark DataFrame</h3><ul>
<li>Cluster Parallel:集群并行执行</li>
<li>Lazy Evaluations:只有action才会触发Transformation的执行</li>
<li>Immutable：不可更改</li>
<li>Pandas rich API：比Spark SQL api丰富</li>
</ul>
<h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><ul>
<li>创建DataFrame</li>
</ul>
<p>调用方法例如：spark.read.xxx方法</p>
<ul>
<li><p>其他方式创建dataframe</p>
<ul>
<li>createDataFrame：pandas dataframe、list、RDD</li>
<li>数据源：RDD、csv、json、parquet、orc、jdbc</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jsondf &#x3D; spark.read.json(&quot;xxx.json&quot;)</span><br><span class="line">jsondf &#x3D; spark.read.format(&quot;json&quot;).load(&quot;xxx.json&quot;)</span><br><span class="line">parquetdf &#x3D; spark.read.parquet(&quot;xxx.parquet&quot;)</span><br><span class="line">jdbcDF &#x3D; spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;db_name&quot;).option(&quot;dbtable&quot;,&quot;table_name&quot;).option(&quot;user&quot;,&quot;xxx&quot;).option(&quot;password&quot;,&quot;xxx&quot;).load()</span><br></pre></td></tr></table></figure></li>
<li><p>Transformation:延迟性操作</p>
</li>
<li><p>action：立即操作</p>
</li>
</ul>
<p><img src="/blog/img/Spark_6.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"><span class="comment">## 直接创建</span></span><br><span class="line">list_rdd = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">15</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="number">20</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">25</span>)]</span><br><span class="line">rdd = sc.parallelize(list_rdd)</span><br><span class="line"><span class="comment">#为数据添加列名</span></span><br><span class="line">people = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: Row(name=x[<span class="number">0</span>], age=<span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line"><span class="comment"># 创建DataFrame</span></span><br><span class="line">df = spark.createDataFrame(people)</span><br><span class="line"><span class="comment"># 显示数据结构</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment"># 显示前10条数据</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment"># 统计行数</span></span><br><span class="line">df.count()</span><br><span class="line"><span class="comment"># 显示列名</span></span><br><span class="line">df.columns</span><br><span class="line"><span class="comment"># 增加一个新列，如果列名是原本就有的，就会替换原有列</span></span><br><span class="line">df.withColumn(<span class="string">&quot;列名&quot;</span>，df.age*<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 删除列</span></span><br><span class="line">df.drop(<span class="string">&#x27;列名&#x27;</span>).show()</span><br><span class="line"><span class="comment"># 提取部分列</span></span><br><span class="line">df.select(<span class="string">&#x27;列名1&#x27;</span>，<span class="string">&#x27;列名2&#x27;</span>).show()</span><br><span class="line"><span class="comment"># 分组统计</span></span><br><span class="line">df.groupby().age(&#123;<span class="string">&#x27;列名1&#x27;</span>:<span class="string">&#x27;函数名&#x27;</span>, <span class="string">&#x27;列名2&#x27;</span>:<span class="string">&#x27;函数名&#x27;</span>&#125;).show()</span><br><span class="line"><span class="comment"># 自带函数</span></span><br><span class="line"><span class="comment"># avg(), count(), countDistinct(), first(), kurtosis(),</span></span><br><span class="line"><span class="comment"># max(), mean(), min(), skewness(), stddev(), stddev_pop(),</span></span><br><span class="line"><span class="comment"># stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() variance()</span></span><br><span class="line"><span class="comment"># 自定义的汇总方法</span></span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line"><span class="comment"># 调用函数并起一个别名</span></span><br><span class="line">df.agg(fn.count(<span class="string">&#x27;SepalWidth&#x27;</span>).alias(<span class="string">&#x27;width_count&#x27;</span>),fn.countDistinct(<span class="string">&#x27;cls&#x27;</span>).alias(<span class="string">&#x27;distinct_cls_count&#x27;</span>)).show()</span><br><span class="line"><span class="comment"># 按比例拆分数据集</span></span><br><span class="line">trainDF，testDF = df.randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line"><span class="comment"># 采样数据sample(是否有放回的采样，采样比例，随机种子)</span></span><br><span class="line">df.sample(<span class="literal">False</span>, <span class="number">0.2</span>, <span class="number">100</span>).show()</span><br><span class="line"><span class="comment"># 查看两个数据集在类别上的差异</span></span><br><span class="line">testDF.select(<span class="string">&#x27;cls&#x27;</span>).subtract(trainDF.select(<span class="string">&#x27;cls&#x27;</span>)).distinct().count()</span><br><span class="line"><span class="comment"># 交叉表</span></span><br><span class="line">df.crosstab(<span class="string">&#x27;cls&#x27;</span>,<span class="string">&#x27;SepalLength&#x27;</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#序列化选择kryo</span><br><span class="line">#内存管理</span><br><span class="line">spark.memory.fraction&#x3D;0.6  #设置计算和存储的内存使用率</span><br><span class="line">spark.memory.storageFraction&#x3D;0.5  #设置存储内存避免被驱逐的比例</span><br><span class="line">                                  #查看ddr每次使用</span><br><span class="line"># 广播变量</span><br><span class="line">broadcast_var &#x3D; sc.broadcast([1,2,3])  #会在每台机器上保存这个变量，之后使用可以直接在本机上获取</span><br><span class="line"># 数据本地性 同jvm，同node，同机架，同网络</span><br><span class="line">spark.locality.wait&#x3D;3s   #等待节点资源时间</span><br><span class="line"></span><br><span class="line">--master yarn-cluster (or yarn-client)(集群模式)</span><br><span class="line">--num-executors 50 (Executor个数)</span><br><span class="line">--executor-memory 6G  （每个Executor进程的内存）</span><br><span class="line">--conf spark.executor.cores&#x3D;4   （每个Executor进程的CPU core数量）</span><br><span class="line">--conf spark.yarn.executor.memoryOverhead&#x3D;2048（Ececutor堆外内存）</span><br><span class="line">--driver-memory 2G （Driver进程的内存）</span><br><span class="line">--conf spark.default.parallelism&#x3D;150 （分区个数）</span><br><span class="line">--conf spark.dynamicAllocation.enable&#x3D;true &#x2F;&#x2F;打开动态executor模式</span><br><span class="line">--conf spark.shuffle.service.enabled&#x3D;true &#x2F;&#x2F;动态executor需要的服务，需要和上面的spark.dynamicAllocation.enable同时打开</span><br><span class="line">--conf spark.storage.memoryFraction&#x3D;0.2（持久化数据在Executor内存占比）</span><br><span class="line">exector、storage内存分配</span><br><span class="line">--executor-memory 10G</span><br><span class="line">--conf spark.shuffle.memoryFraction&#x3D;0.5</span><br><span class="line">--conf spark.sql.shuffle.partitions&#x3D;20  （shuffle后partition个数）</span><br><span class="line">--conf spark.shuffle.compress&#x3D;true  （shuffle过程是否压缩）</span><br><span class="line">--conf spark.shuffle.file.buffer&#x3D;512   （数据写入磁盘前buffer缓冲）</span><br><span class="line">--conf spark.reducer.maxSizeInFlight&#x3D;256m</span><br><span class="line">--conf spark.shuffle.io.maxRetries&#x3D;20  （网络问题重试次数）</span><br><span class="line">--spark.shuffle.io.retryWait&#x3D;5s    （重试间隔时间）</span><br></pre></td></tr></table></figure>
<h3 id="提交Python环境到spark集群执行Python脚本"><a href="#提交Python环境到spark集群执行Python脚本" class="headerlink" title="提交Python环境到spark集群执行Python脚本"></a>提交Python环境到spark集群执行Python脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将conda环境包打包成压缩包 ./envs/environment   打包成environment.zip</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以使用下面的命令执行python脚本了</span></span><br><span class="line"></span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--archives  hdfs://yumcluster/tmp/zoubin1/environment.zip#environment \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/environment/bin/python3.6 \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=./environment/environment/bin/python3.6 \</span><br><span class="line">--conf spark.executorEnv.PYSPARK_PYTHON=./environment/environment/bin/python3.6 \</span><br><span class="line">--conf spark.executorEnv.PYSPARK_DRIVER_PYTHON=./environment/environment/bin/python3.6 \</span><br><span class="line"> aa.py</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h1><h3 id="SparkStreaming概述"><a href="#SparkStreaming概述" class="headerlink" title="SparkStreaming概述"></a>SparkStreaming概述</h3><ul>
<li>它是一个可扩展，高吞吐具有容错性的流式计算框架</li>
</ul>
<p>spark-core和spark-sql都是处理属于离线批处理任务。数据一般都是在固定位置上，通常我们写好一个脚本，每天定时取处理数据、计算、保存数据结果。但是有些任务是需要实时处理的，仅仅能够容忍的延迟1秒内。</p>
<h3 id="实时计算框架对比"><a href="#实时计算框架对比" class="headerlink" title="实时计算框架对比"></a>实时计算框架对比</h3><ul>
<li>Storm<ul>
<li>流式计算框架</li>
<li>以record为单位处理数据</li>
<li>也支持micro-batch方式(Trident)</li>
</ul>
</li>
<li>Spark<ul>
<li>批处理计算框架</li>
<li>以RDD为单位处理数据</li>
<li>支持micro-batch流式处理数据(Spark Streaming)</li>
</ul>
</li>
<li>对比<ul>
<li>吞吐量：Spark Streaming优于Storm</li>
<li>延时：Spark Streaming差于Storm</li>
</ul>
</li>
</ul>
<h3 id="SparkStreaming组件"><a href="#SparkStreaming组件" class="headerlink" title="SparkStreaming组件"></a>SparkStreaming组件</h3><ul>
<li>Streaming Context<ul>
<li>一旦一个Context已经启动(调用了Streaming Context的start()),就不能有新的流算子(Dstream)建立或者是添加到context中</li>
<li></li>
<li>一旦一个context已经停止,不能重新启动(Streaming Context调用了stop方法之后 就不能再次调 start())</li>
<li>在JVM(java虚拟机)中, 同一时间只能有一个Streaming Context处于活跃状态, 一个SparkContext创建一个Streaming Context</li>
<li>在Streaming Context上调用Stop方法, 也会关闭SparkContext对象, 如果只想仅关闭Streaming Context对象,设置stop()的可选参数为false</li>
<li>一个SparkContext对象可以重复利用去创建多个Streaming Context对象(不关闭SparkContext前提下), 但是需要关一个再开下一个</li>
</ul>
</li>
<li>DStream (离散流)<ul>
<li>代表一个连续的数据流</li>
<li>在内部, DStream由一系列连续的RDD组成</li>
<li>DStreams中的每个RDD都包含确定时间间隔内的数据</li>
<li>任何对DStreams的操作都转换成了对DStreams隐含的RDD的操作</li>
<li>数据源<ul>
<li>基本源<ul>
<li>TCP/IP Socket</li>
<li>FileSystem</li>
</ul>
</li>
<li>高级源<ul>
<li>Kafka</li>
<li>Flume</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;python解释器路径&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;jdk路径&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;spark路径&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(<span class="string">&quot;local[2]&quot;</span>,appName=<span class="string">&quot;NetworkWordCount&quot;</span>)</span><br><span class="line">    <span class="comment">#参数2：指定执行计算的时间间隔</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#监听ip，端口上的上的数据</span></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;localhost&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment">#将数据按空格进行拆分为多个单词</span></span><br><span class="line">    words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">#将单词转换为(单词，1)的形式</span></span><br><span class="line">    pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#统计单词个数</span></span><br><span class="line">    wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)</span><br><span class="line">    <span class="comment">#打印结果信息，会使得前面的transformation操作执行</span></span><br><span class="line">    wordCounts.pprint()</span><br><span class="line">    <span class="comment">#启动StreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">#等待计算结束</span></span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<h1 id="Spark-Streaming的状态操作"><a href="#Spark-Streaming的状态操作" class="headerlink" title="Spark Streaming的状态操作"></a>Spark Streaming的状态操作</h1><ul>
<li>在Spark Streaming中存在两种状态操作<ul>
<li>UpdateStateByKey</li>
<li>Windows操作</li>
</ul>
</li>
<li>使用有状态的transformation，需要开启Checkpoint<ul>
<li>Spark streaming的容错机制</li>
<li>它将足够多的信息checkpoint到某些具备容错性的存储系统，如HDFS上，以便出错时能够迅速恢复</li>
</ul>
</li>
</ul>
<h3 id="updateStateByKey"><a href="#updateStateByKey" class="headerlink" title="updateStateByKey"></a>updateStateByKey</h3><ul>
<li>Spark Streaming实现的时一个实时批处理操作，每隔一段时间将数据进行打包，封装成RDD，是无状态的。</li>
<li>如果我们需要拿一天的数据来进行离线处理，我们得把rdd数据放让mysql中，再取再进行计算，而updateStateByKey可以解决这种问题</li>
</ul>
<p>监听网络端口的数据，获取到每个批次的出现的单词数量，并且需要把每个批次的信息保留下来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;python解释器路径&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;java路径&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;spark路径&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SparkContext</span></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#开启检查点</span></span><br><span class="line">ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义state更新函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span>(<span class="params">new_values, last_sum</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(new_values) + (last_sum <span class="keyword">or</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="comment"># 对数据以空格进行拆分，分为多个单词</span></span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">    .updateStateByKey(updateFunc=updateFunc)<span class="comment">#应用updateStateByKey函数</span></span><br><span class="line"></span><br><span class="line">counts.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><ul>
<li><p>窗口长度L：运算的数据量</p>
</li>
<li><p>滑动间隔G：控制每隔多长时间做一次运算     </p>
</li>
</ul>
<p>每隔G秒，统计最近L秒的数据</p>
<p><img src="/blog/img/Spark_7.png"></p>
<ul>
<li>操作细节<ul>
<li>Window操作是基于窗口长度和滑动间隔来工作的</li>
<li>窗口的长度控制考虑前几批次数据量</li>
<li>默认为批处理的滑动间隔来确定计算结果的频率           </li>
</ul>
</li>
<li>相关函数</li>
</ul>
<p><img src="/blog/img/Spark_8.png"></p>
<p>reduceByKeyAndWindow(func,invFunc,windowLength,slidelnterval,[num,Tasks])</p>
<p>func:正向操作，类似于updateStateByKey</p>
<p>invFunc:反向操作                                                                                                                                                                                                                                                                                      </p>
<p>监听网络端口的数据，每隔3秒统计前6秒出现的单词数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;python解释器&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;java路径&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;spark路径&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_countryname</span>(<span class="params">line</span>):</span></span><br><span class="line">    country_name = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> country_name == <span class="string">&#x27;usa&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;USA&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> country_name == <span class="string">&#x27;ind&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;India&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> country_name == <span class="string">&#x27;aus&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;Australia&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = <span class="string">&#x27;Unknown&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment">#定义处理的时间间隔</span></span><br><span class="line">    batch_interval = <span class="number">1</span> <span class="comment"># base time unit (in seconds)</span></span><br><span class="line">    <span class="comment">#定义窗口长度</span></span><br><span class="line">    window_length = <span class="number">6</span> * batch_interval</span><br><span class="line">    <span class="comment">#定义滑动时间间隔</span></span><br><span class="line">    frequency = <span class="number">3</span> * batch_interval</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取StreamingContext</span></span><br><span class="line">    spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    ssc = StreamingContext(sc, batch_interval)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#需要设置检查点</span></span><br><span class="line">    ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;localhost&#x27;</span>, <span class="number">9999</span>)</span><br><span class="line">    addFunc = <span class="keyword">lambda</span> x, y: x + y</span><br><span class="line">    invAddFunc = <span class="keyword">lambda</span> x, y: x - y</span><br><span class="line">    <span class="comment">#调用reduceByKeyAndWindow，来进行窗口函数的调用</span></span><br><span class="line">    window_counts = lines.<span class="built_in">map</span>(get_countryname) \</span><br><span class="line">        .reduceByKeyAndWindow(addFunc, invAddFunc, window_length, frequency)</span><br><span class="line">    <span class="comment">#输出处理结果信息</span></span><br><span class="line">    window_counts.pprint()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 zoubinbf@163.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>




    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/blog/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: '8f7834ddf48857ecc43b',
            clientSecret: '8838069823d5a94ca99a6bffb6f8bcaf08593f91',
            repo: 'blog',
            owner: 'bufan-zb',
            admin: ['bufan-zb'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2016-2020 BuFan
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/blog/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/blog/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/blog/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/blog/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().trim().split('\n').length, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
        /* 渲染*/
        function HTMLDecode(text) {
            var temp = document.createElement("div");
            temp.innerHTML = text;
            var output = temp.innerText || temp.textContent;
            temp = null;
            return output;
        }
        if (window.mermaid){
            window.mermaid = null
        }
        $.getScript("//cdn.jsdelivr.net/npm/mermaid@8.4.2/dist/mermaid.min.js", function () {
            var mermaidOptions = JSON.parse(HTMLDecode("{&#34;theme&#34;:&#34;default&#34;,&#34;startOnLoad&#34;:true,&#34;flowchart&#34;:{&#34;useMaxWidth&#34;:false,&#34;htmlLabels&#34;:true}}"))
            if (window.mermaid) {
                mermaid.initialize(mermaidOptions)
                mermaid.contentLoaded()
            }
        })
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("/bloghttps://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
